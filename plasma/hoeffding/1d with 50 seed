# hoeffding 방법에서도 선형 보간을 이용해야 함. test data 는 마지막까지 보지 않아야 함.
library(ggplot2)
library(gridExtra)
library(ggpubr)

# truncation 이용해 수정한 find_lambda_hat 함수 
find_lambda_hat <- function(alpha, delta, y, pred, variance, n,
                            lower = 0, upper = 1e+5, step = 1e-3) {
  
  threshold <- alpha - sqrt(log(1/delta) / (2*n))
  threshold <- max(0, threshold) #truncation
  
  lambda_seq <- seq(lower, upper, by = step)
  
  for (lambda in lambda_seq) {
    # empirical risk 계산
    risk <- mean(
      y <= pred - sqrt(variance) * lambda |
        y >= pred + sqrt(variance) * lambda,
      na.rm = TRUE
    )
    
    # 조건을 만족하는 가장 첫 λ 반환
    if (risk <= threshold) {
      return(lambda)
    }
  }
  
  # 조건 만족하는 lambda가 없을 경우
  return(NA)
}

data <- read.csv('~/Desktop/plasma.csv')
# 사용할 변수 목록
target_vars <- c("BETADIET", "BETAPLASMA", "CALORIES", "CHOLESTEROL", "FAT", "FIBER", "QUETELET", "RETDIET", "RETPLASMA")

alpha <- 0.10 #risk의 상한 
delta <- 0.05 #1-confidence level

# 결과를 저장할 리스트
all_results <- list()
for (s in 1:50) {
  cat("Seed", s, "Start, \n")
  set.seed(s)
  
  run_results <- lapply(seq_along(target_vars), function(i) {
    y_var <- target_vars[i]
    x <- data$AGE
    y <- data[[y_var]]
    
    m <- nrow(data)
    n_train <- 100
    n_cal <- 100
    n_test <- m - n_train - n_cal
    
    # 데이터 샘플링
    all_indices <- sample(seq_len(m))
    train_index <- all_indices[1:n_train]
    cal_index <- all_indices[(n_train + 1):(n_train + n_cal)]
    test_index <- all_indices[(n_train + n_cal + 1):m]
    
    train_x <- x[train_index]
    train_y <- y[train_index]
    cal_x <- x[cal_index]
    cal_y <- y[cal_index]
    test_x <- x[test_index]
    test_y <- y[test_index]
    
    # 1. 훈련 데이터를 이용해 모델 학습 (평균 예측값 계산)
    fit <- smooth.spline(x = train_x, y = train_y, cv = FALSE)
    cal_mean_pred <- predict(fit, cal_x)$y  # mean prediction
    
    # 2. Calibration 데이터로 예측 오차의 분산 추정 (log scale로)
    train_res <- train_y - predict(fit, train_x)$y
    log_res2 <- log(train_res^2 + 1e-8)
    gam_model <- gam(log_res2 ~ s(train_x))
    log_pred <- predict(gam_model, newdata = data.frame(train_x = cal_x))
    cal_variance <- exp(log_pred) # 분산 추정값
    
    # 3. Tolerance Interval 계산 (분산을 포함한 계산)
    # 예시로 lambda_hat는 보통 특정 확률값을 기준으로 계산
    lambda_hat <- find_lambda_hat(alpha,delta , cal_y, cal_mean_pred, cal_variance, length(cal_y))
    
    TI_upper <- cal_mean_pred + sqrt(cal_variance) * lambda_hat
    TI_lower <- cal_mean_pred - sqrt(cal_variance) * lambda_hat
    
    # 4. 선형 보간을 이용해 test 데이터가 TI 구간에 포함되는지 계산 (PICP 계산)
    TI_lower_df <- aggregate(TI_lower ~ cal_x, FUN = mean)
    TI_upper_df <- aggregate(TI_upper ~ cal_x, FUN = mean)
    
    # 선형 보간
    interp_lower <- approx(x = TI_lower_df$cal_x, y = TI_lower_df$TI_lower, xout = test_x)$y
    interp_upper <- approx(x = TI_upper_df$cal_x, y = TI_upper_df$TI_upper, xout = test_x)$y
    
    # test_x에 대해 예측값이 TI 구간에 포함되었는지 여부 확인
    included <- (test_y >= interp_lower) & (test_y <= interp_upper)
    coverage_flag <- mean(included, na.rm = TRUE) >= 0.90
    
    # NMPIW 계산
    interval_width <- interp_upper - interp_lower
    y_range <- max(test_y) - min(test_y)
    NMPIW <- mean(interval_width, na.rm = TRUE) / y_range
    
    # 결과를 반환 
    return(data.frame(
      Variable = y_var,
      Seed = s,
      Covered90 = coverage_flag,
      NMPIW = NMPIW
    ))
    
  })
  
  # run_results 리스트에서 각 시드에 대한 결과를 all_results에 저장
  all_results <- append(all_results, run_results)
}

# all_results를 데이터 프레임으로 변환하여 보기 좋게 저장
final_results_df <- do.call(rbind, all_results)

# 결과 출력
print(final_results_df)

# 각 변수별로 평균 PICP와 NMPIW 계산
library(dplyr)
mean_results_df <- final_results_df %>%
  group_by(Variable) %>%
  summarise(
    Pct_Covered90 = mean(Covered90, na.rm = TRUE),
    Mean_NMPIW = mean(NMPIW, na.rm = TRUE)
  )


# 평균값 출력
print(mean_results_df)
