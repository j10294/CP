#-----------------------------------------------------------# 필요한 함수 ##
compute_norm <- function(vector) {
  return(sqrt(sum(vector^2)))
}

find_k_factor <- function(nu, norm_lx_h, P = 0.90, gamma = 0.95) {
  obj_func <- function(k) {
    integral_result <- tryCatch({
      integrate(integrand, lower = 0, upper =50, k = k, nu = nu, P = P, norm_lx_h = norm_lx_h)$value
    }, error = function(e) {
      warning("적분 실패")
      #return(1e-6) 적분에 실패하면 오류 메시지만 출력되도록 설정
    })
    pi_term <- sqrt(2 / (pi * norm_lx_h^2))
    return(abs((pi_term * integral_result) - gamma)) # 이 차이가 0에 가까워지는 k factor를 찾고자 함
  }
  tryCatch({
    return(optimize(obj_func, interval = c(0.01, 10))$minimum) # 범위는 0부터 5까지로 설정
  }, error = function(e) {
    warning("find_k_factor 계산 실패: 근사값 사용")
    return(sqrt(nu*qchisq(p=P, df=1, ncp=norm_lx_h^2)/qchisq(p=1-gamma, df=m))) 
  })
}
#-----------------------------------------------------------# 필요한 함수 ##

library(gam)
library(MASS)

data <- read.csv('/Users/hdmt306/Downloads/plasma.csv')
exclude_vars <- c("SEX", "SMOKSTAT", "VITUSE", "ALCOHOL", "BETADIET", "BETAPLASMA", "RETPLASMA")
numeric_vars <- names(data)[sapply(data, is.numeric)]
target_vars <- setdiff(numeric_vars, c("AGE", exclude_vars))

# 결과를 저장할 리스트
results_list <- list()

for (y_var in target_vars) {
  cat('변수:', y_var, '계산 중 ...\n')
  
  picp_vals <- numeric(50)
  nmpiw_vals <- numeric(50)
  
  for (seed in 1:50) {
    cat('  Seed:', seed, '계산 중 ...\n')

    x <- data$AGE
    y <- data$y_var
    
    set.seed(seed)
    m <- length(y)
    n_test <- 115
    n_train <- m - n_test
    
    idx <- sample(seq_len(m))
    train_idx <- idx[1:n_train]
    test_idx <- idx[(n_train+1):m]
    
    train_x <- x[train_idx]
    raw_train_y <- data[[y_var]][train_idx]
    test_x <- x[test_idx]
    raw_test_y <- data[[y_var]][test_idx]
    
    # 3. train data 를 이용해 분산 추정 후 데이터 변환
    
    # 평균 추정
    raw_fit <- smooth.spline(train_x, raw_train_y, cv=FALSE)
    raw_pred_y <- predict(raw_fit, train_x)$y
    raw_residuals <- raw_train_y - raw_pred_y
    
    # 분산 추정
    variance_fit <- gam(raw_residuals^2 ~ s(train_x))
    var_list <- predict(variance_fit,newdata = data.frame(train_x = x)) #315개의 추정된 분산을 가진 벡터
    
    # y값 변화
    transform_y <- data[[y_var]] / sqrt(var_list); transform_y # 표준편차로 나누어 표준화시킨 데이터
    
    # 새로운 train_y, test_y 정의
    train_y <- transform_y[train_idx]
    test_y <- transform_y[test_idx]
    
    # 4. Paper method 이용해서 단일 분산 추정
    fit <- smooth.spline(train_x, train_y, cv=FALSE)
    residuals <- predict(fit, train_x)$y - train_y
    
    B <- bs(train_x, df=max(fit$df), 3)
    D <- diff(diag(ncol(B)), differences=2)
    S_inv <- ginv(t(B) %*% B + fit$lambda * t(D) %*% D)
    smoother_matrix <- B %*% S_inv %*% t(B)
    I_n <- diag(length(train_x))
    residual_matrix <- I_n - smoother_matrix
    
    estimated_variance <- (t(residuals) %*% residuals) / (sum(diag(t(residual_matrix) %*% residual_matrix)))
    cat('추정된 단일 분산 : ', estimated_variance)
    
    # 5. k factor 계산
    norm_lx_h_values <- numeric(length(train_x)) #200개 
    for (j in 1:length(train_x)){
      x_h <- train_x[j]
      smoothing_vector <- smoother_matrix[j,]
      norm_lx_h_values[j] <- compute_norm(smoothing_vector)
    }
    
    numerator <- sum(diag(t(residual_matrix) %*% residual_matrix))^2
    denominator <- sum(diag((t(residual_matrix) %*% residual_matrix)^2))
    nu <- numerator / denominator
    
    k_factors <- sapply(norm_lx_h_values, function(nlh) find_k_factor(nu=nu, norm_lx_h = nlh)) # 계산이 안되서 근사값 정리함.
    
    # 6. TI 계산
    pred_y <- predict(fit,train_x)$y
    TI_upper <- pred_y + k_factors * c(estimated_variance)
    TI_lower <- pred_y - k_factors * c(estimated_variance)
    
    # 7. raw_y 에 대한 TI로 변환
    raw_TI_upper <- TI_upper * sqrt(var_list[train_idx])
    raw_TI_lower <- TI_lower * sqrt(var_list[train_idx])
    train_coverage <- mean(raw_train_y >= raw_TI_lower & raw_train_y <= raw_TI_upper)
    cat('훈련 세트의 커버리지 평균 : ', train_coverage)
    
    # 8. 모집단을 포함하는지 test_x, test_y 넣어서 시각화
    library(ggplot2)
    plot_df <- data.frame(
      x = train_x,
      pred_y = predict(fit, train_x)$y * sqrt(var_list[train_idx]),  # 원래 스케일로 예측값 변환
      TI_lower = raw_TI_lower,
      TI_upper = raw_TI_upper
    )
    
    test_plot_df <- data.frame(
      x = test_x,
      y = raw_test_y
    )
    
    ggplot() +
      geom_point(data=test_plot_df, aes(x=x, y=y), color='black', alpha=0.7) +
      geom_line(data=plot_df, aes(x=x, y=pred_y), color='blue', size=1) +
      geom_line(data=plot_df, aes(x=x, y=TI_upper), color='red', linetype='dashed') +
      geom_line(data=plot_df, aes(x=x, y=TI_lower), color='red', linetype='dashed') +
      geom_ribbon(data = plot_df, aes(x = x, ymin = TI_lower, ymax = TI_upper), fill = "red", alpha = 0.2) +
      labs(title = paste('Test obs with train-based TI for', y_var))
    
    # 9. 선형 보간을 이용해 test 관측값이 TI(train-based)에 얼마나 포함되는지 계산
    TI_lower_df <- aggregate(raw_TI_lower ~ train_x, FUN = mean)
    TI_upper_df <- aggregate(raw_TI_upper ~ train_x, FUN = mean)
    
    # 보간 수행
    interp_lower <- approx(x = TI_lower_df$train_x, y = TI_lower_df$raw_TI_lower, xout = test_x)$y
    interp_upper <- approx(x = TI_upper_df$train_x, y = TI_upper_df$raw_TI_upper, xout = test_x)$y
    
    # 포함 여부
    included <- (raw_test_y >= interp_lower) & (raw_test_y <= interp_upper)
    
    # 포함 개수 및 비율
    num_included <- sum(included)
    total_test <- length(test_x)
    PICP <- num_included / total_test
    
    # NMPIW
    interval_width <- interp_upper - interp_lower
    y_range <- max(raw_test_y) - min(raw_test_y)
    NMPIW <- mean(interval_width) / y_range
    
    # 결과 저장
    results_list[[paste(y_var, "seed_", seed, sep = "_")]] <- list(
      PICP = PICP,
      NMPIW = NMPIW
    )
  }
  
  # 결과 출력: 변수별 평균 PICP, NMPIW 계산
  picp_vals <- sapply(results_list, function(res) res$PICP)
  nmpiw_vals <- sapply(results_list, function(res) res$NMPIW)
  
  mean_PICP <- mean(picp_vals, na.rm = TRUE)
  mean_NMPIW <- mean(nmpiw_vals, na.rm = TRUE)
  
  cat("✅", y_var, "변수의 평균 PICP:", mean_PICP, "\n")
  cat("✅", y_var, "변수의 평균 NMPIW:", mean_NMPIW, "\n")
}

# results_list를 이용해서 변수별로 시드에 대한 결과 정리
final_results_list <- list()

for (y_var in target_vars) {
  cat('변수:', y_var, '계산 중 ...\n')
  
  picp_vals <- numeric(50)
  nmpiw_vals <- numeric(50)
  
  # results_list에서 해당 변수에 대한 결과를 추출하여 시드별로 저장
  for (seed in 1:50) {
    result_key <- paste(y_var, "seed_", seed, sep = "_")
    
    if (result_key %in% names(results_list)) {
      picp_vals[seed] <- results_list[[result_key]]$PICP
      nmpiw_vals[seed] <- results_list[[result_key]]$NMPIW
    }
  }
  
  # 변수별로 결과를 데이터 프레임으로 정리
  var_results_df <- data.frame(
    Variable = rep(y_var, 50),
    Seed = 1:50,
    PICP = picp_vals,
    NMPIW = nmpiw_vals
  )
  
  final_results_list[[y_var]] <- var_results_df
}

# 모든 변수에 대한 결과를 하나로 합치기
final_results_df <- do.call(rbind, final_results_list)

# 결과 출력
print(final_results_df)

# 전체 평균 계산 (각 변수별로 평균 계산)
library(dplyr)
mean_results_df <- final_results_df %>%
  group_by(Variable) %>%
  summarise(
    Mean_PICP = mean(PICP, na.rm = TRUE),
    Mean_NMPIW = mean(NMPIW, na.rm = TRUE)
  )

# 평균값 출력
print(mean_results_df)
