#-----------------------------------------------------------# 필요한 함수 ##
# 벡터의 L2 norm 계산 함수
compute_norm <- function(vector) {
  return(sqrt(sum(vector^2)))
}

# 확률 계산 함수 (Prop 3.1 내부)
compute_probability <- function(nu, t, P, k, norm_lx_h) {
  tryCatch({
    q_val <- qchisq(P, df = 1, ncp = t^2) #qchisq value 설정
    lhs <- q_val / ((t * sqrt(5 / nu)) + 1) #이 값이 k^2보다 크면, probability가 0으로 감.
    
    if (lhs <= k^2) {
      threshold <- (nu * q_val) / (k^2)
      probability <- pchisq(threshold, df = nu, lower.tail = FALSE) #probability term
      
      if (is.na(probability) || is.nan(probability)) return(1e-6)
      return(probability)
    } else {
      return(0) #이 값이 k^2보다 크면, probability가 0으로 감.
    }
  }, error = function(e) {
    warning("compute_probability 계산 오류 발생")
    return(0)
  })
}

# 적분 함수
integrand <- function(t, k, nu, P, norm_lx_h) {
  exp_term <- exp(-t^2 / (2 * norm_lx_h^2))
  if (exp_term < 1e-5) return(0) #exp term 이 너무 작으면 그냥 0 반환

  prob_term <- compute_probability(nu, t, P, k, norm_lx_h)
  return(exp_term * prob_term)
}

find_upper_bound_t <- function(k, nu, P) {
  t_seq <- seq(0.01, 100, by = 0.05)  # 탐색 구간
  valid_t <- sapply(t_seq, function(t) {
    q_val <- qchisq(P, df = 1, ncp = t^2)
    lhs <- q_val / ((t * sqrt(5 / nu)) + 1)
    return(lhs <= k^2)
  })
  
  if (any(valid_t)) {
    return(max(t_seq[valid_t]))
  } else {
    warning("조건을 만족하는 t가 없음 — fallback to t = 10")
    return(10)
  }
}

# k-factor 계산 함수
find_k_factor <- function(nu, norm_lx_h, P = 0.90, gamma = 0.95) {
  obj_func <- function(k) {
    upper_bound <- find_upper_bound_t(k, nu, P)
    
    if (is.na(upper_bound) || is.infinite(upper_bound) || upper_bound < 1e-6) {
      warning("upper_bound가 이상함 → fallback")
      return(Inf) #upperbound 상한값을 inf로 설정
    }
    
    result <- tryCatch({
      integral_result <- integrate(integrand, lower = 0, upper = upper_bound,
                                   k = k, nu = nu, P = P, norm_lx_h = norm_lx_h)$value
      if (is.na(integral_result) || is.nan(integral_result)) return(Inf)
      pi_term <- sqrt(2 / (pi * norm_lx_h^2))
      error <- abs((pi_term * integral_result) - gamma)
      
      cat("k =", round(k, 5), 
          ", integral =", round(estimate, 5), 
          ", error =", round(error, 5), "\n")
      
    }, error = function(e) {
      warning("적분 실패")
      return(Inf)
    })
    
    return(result)
  }
  
  
  tryCatch({
    return(optimize(obj_func, interval = c(0.01, 5), tol = 1e-2)$minimum)
  }, error = function(e) {
    warning("k 최적화 실패")
    return(sqrt(nu*qchisq(p=0.90, df=1, ncp=norm_lx_h^2)/qchisq(p=1-gamma, df=nu, ncp=0))) # 최적화 실패 시 근사값 사용
  })
}

#-----------------------------------------------------------# 필요한 함수 ##

library(gam)
library(MASS)
data <- read.csv('/Users/hdmt306/Downloads/plasma.csv')
exclude_vars <- c("SEX", "SMOKSTAT", "VITUSE", "ALCOHOL", "BETADIET", "BETAPLASMA", "RETPLASMA")
numeric_vars <- names(data)[sapply(data, is.numeric)]
target_vars <- setdiff(numeric_vars, c("AGE", exclude_vars))

# 결과를 저장할 리스트
results_list <- list()

for (y_var in target_vars) {
  cat('변수:', y_var, '계산 중 ...\n')
  
  picp_vals <- numeric(50)
  nmpiw_vals <- numeric(50)
  
  for (seed in 1:50) {
    #cat('  Seed:', seed, '계산 중 ...\n')
    
    x <- data$AGE
    y <- data$y_var
    
    m <- 315
    n_test <- 115
    n_train <- m - n_test
    
    set.seed(seed)
    idx <- sample(seq_len(m))
    train_idx <- idx[1:n_train]
    test_idx <- idx[(n_train+1):m]
    
    train_x <- x[train_idx]
    raw_train_y <- data[[y_var]][train_idx]
    test_x <- x[test_idx]
    raw_test_y <- data[[y_var]][test_idx]
    
    # 3. train data 를 이용해 분산 추정 후 데이터 변환
    
    # 평균 추정
    raw_fit <- smooth.spline(train_x, raw_train_y, cv=FALSE)
    raw_pred_y <- predict(raw_fit, train_x)$y
    raw_residuals <- raw_train_y - raw_pred_y
    
    # 분산 추정 (로그변환 후 추정)
    log_res2 <- log(raw_residuals^2 + 1e-20)
    variance_fit <- gam(log_res2 ~ s(train_x))
    var_list <- exp(predict(variance_fit,newdata = data.frame(train_x = train_x))) #200개의 추정된 분산을 가진 벡터
    
    # y값 변화
    train_y <- raw_train_y / sqrt(var_list) # 표준편차로 나누어 표준화시킨 데이터
    
    # 4. Paper method 이용해서 단일 분산 추정
    fit <- smooth.spline(train_x, train_y, cv=FALSE)
    residuals <- predict(fit, train_x)$y - train_y
    
    B <- bs(train_x, df=max(fit$df), 3)
    D <- diff(diag(ncol(B)), differences=2)
    S_inv <- ginv(t(B) %*% B + fit$lambda * t(D) %*% D)
    smoother_matrix <- B %*% S_inv %*% t(B)
    I_n <- diag(length(train_x))
    residual_matrix <- I_n - smoother_matrix
    
    estimated_variance <- (t(residuals) %*% residuals) / (sum(diag(t(residual_matrix) %*% residual_matrix)))
    #cat('추정된 단일 분산 : ', estimated_variance)
    
    # 5. k factor 계산
    norm_lx_h_values <- numeric(length(train_x)) #200개 
    for (j in 1:length(train_x)){
      x_h <- train_x[j]
      smoothing_vector <- smoother_matrix[j,]
      norm_lx_h_values[j] <- compute_norm(smoothing_vector)
    }
    
    numerator <- sum(diag(t(residual_matrix) %*% residual_matrix))^2
    denominator <- sum(diag((t(residual_matrix) %*% residual_matrix)^2))
    nu <- numerator / denominator
    
    k_factors <- sapply(norm_lx_h_values, function(nlh) find_k_factor(nu=nu, norm_lx_h = nlh)) # 계산이 안되서 근사값 정리함.
    
    # 6. TI 계산
    pred_y <- predict(fit,train_x)$y
    TI_upper <- pred_y + k_factors * c(estimated_variance)
    TI_lower <- pred_y - k_factors * c(estimated_variance)
    
    # 7. raw_y 에 대한 TI로 변환
    raw_TI_upper <- TI_upper * sqrt(var_list[train_idx])
    raw_TI_lower <- TI_lower * sqrt(var_list[train_idx])
    
    # 8. 모집단을 포함하는지 test_x, test_y 넣어서 시각화
    library(ggplot2)
    plot_df <- data.frame(
      x = train_x,
      pred_y = predict(fit, train_x)$y * sqrt(var_list),  # 원래 스케일로 예측값 변환
      TI_lower = raw_TI_lower,
      TI_upper = raw_TI_upper
    )
    
    test_plot_df <- data.frame(
      x = test_x,
      y = raw_test_y
    )
    
    ggplot() +
      geom_point(data=test_plot_df, aes(x=x, y=y), color='black', alpha=0.7) +
      geom_line(data=plot_df, aes(x=x, y=pred_y), color='blue', size=1) +
      geom_line(data=plot_df, aes(x=x, y=TI_upper), color='red', linetype='dashed') +
      geom_line(data=plot_df, aes(x=x, y=TI_lower), color='red', linetype='dashed') +
      geom_ribbon(data = plot_df, aes(x = x, ymin = TI_lower, ymax = TI_upper), fill = "red", alpha = 0.2) +
      labs(title = paste('Test obs with train-based TI for', y_var))
    
    # 9. 선형 보간을 이용해 test 관측값이 TI(train-based)에 얼마나 포함되는지 계산
    TI_lower_df <- aggregate(raw_TI_lower ~ train_x, FUN = mean)
    TI_upper_df <- aggregate(raw_TI_upper ~ train_x, FUN = mean)
    
    # 보간 수행
    interp_lower <- approx(x = TI_lower_df$train_x, y = TI_lower_df$raw_TI_lower, xout = test_x)$y
    interp_upper <- approx(x = TI_upper_df$train_x, y = TI_upper_df$raw_TI_upper, xout = test_x)$y
    
    # 포함 여부
    included <- (raw_test_y >= interp_lower) & (raw_test_y <= interp_upper)
    
    # 포함 개수 및 비율
    num_included <- sum(included)
    total_test <- length(test_x)
    PICP <- num_included / total_test
    
    # NMPIW
    interval_width <- interp_upper - interp_lower
    y_range <- max(raw_test_y) - min(raw_test_y)
    NMPIW <- mean(interval_width) / y_range
    
    # 결과 저장
    results_list[[paste(y_var, "seed_", seed, sep = "_")]] <- list(
      PICP = PICP,
      NMPIW = NMPIW
    )
  }
  
  # 결과 출력: 변수별 평균 PICP, NMPIW 계산
  picp_vals <- sapply(results_list, function(res) res$PICP)
  nmpiw_vals <- sapply(results_list, function(res) res$NMPIW)
  
  mean_PICP <- mean(picp_vals, na.rm = TRUE)
  mean_NMPIW <- mean(nmpiw_vals, na.rm = TRUE)
  
  cat("✅", y_var, "변수의 평균 PICP:", mean_PICP, "\n")
  cat("✅", y_var, "변수의 평균 NMPIW:", mean_NMPIW, "\n")
}

warnings()
# results_list를 이용해서 변수별로 시드에 대한 결과 정리
final_results_list <- list()

for (y_var in target_vars) {
  cat('변수:', y_var, '계산 중 ...\n')
  
  picp_vals <- numeric(50)
  nmpiw_vals <- numeric(50)
  
  # results_list에서 해당 변수에 대한 결과를 추출하여 시드별로 저장
  for (seed in 1:50) {
    result_key <- paste(y_var, "seed_", seed, sep = "_")
    
    if (result_key %in% names(results_list)) {
      picp_vals[seed] <- results_list[[result_key]]$PICP
      nmpiw_vals[seed] <- results_list[[result_key]]$NMPIW
    }
  }
  
  # 변수별로 결과를 데이터 프레임으로 정리
  var_results_df <- data.frame(
    Variable = rep(y_var, 50),
    Seed = 1:50,
    PICP = picp_vals,
    NMPIW = nmpiw_vals
  )
  
  final_results_list[[y_var]] <- var_results_df
}

# 모든 변수에 대한 결과를 하나로 합치기
final_results_df <- do.call(rbind, final_results_list)

# 결과 출력
print(final_results_df)

# 전체 평균 계산 (각 변수별로 평균 계산)
library(dplyr)
mean_results_df <- final_results_df %>%
  group_by(Variable) %>%
  summarise(
    Mean_PICP = mean(PICP, na.rm = TRUE),
    Mean_NMPIW = mean(NMPIW, na.rm = TRUE)
  )

# 평균값 출력
print(mean_results_df)

