#### 교수님 답장 받고 전부 다시짜기 ####
library(splines)
library(foreach)
library(progressr)
library(doFuture)
library(future)
library(splines)
library(MASS)


# 🔥 2. 필요한 함수 정의

# 데이터 생성 함수 정의
generate_data <- function(model, n) {
  #x <- sort(runif(n, 0, 10)) # x도 랜덤하게 생성
  x <- seq(0, 10, length.out=n)
  base <- 3 * cos(x) - 5 * (x / 15)^2
  
  if (model == 1) {
    y <- base + rnorm(n, mean = 0, sd = sqrt(2))
  }
  if (model == 2) {
    y <- base + rnorm(n, mean = 0, sd = sqrt(2 * x))
  }
  if (model == 3) {
    y <- base + 4 - rgamma(n, shape = 2, scale = 2)
  }
  if (model == 4) {
    y <- base + rt(n, df = 3)
  }
  if (model == 5) {
    sd_x <- 0.3 + 3 * exp(-(x - 5)^2 / (2 * 1.5^2))
    y <- base + rnorm(n, mean = 0, sd = sd_x)
  }
  if (model == 6) {
    y <- base + rnorm(n, mean = 0, sd = 1) + abs(rnorm(n, 0, 1)) - sqrt(2/pi)
  }
  if (model == 7) {
    noise <- ifelse(runif(n) < 0.5, rnorm(n, mean = -3, sd = 1), rnorm(n, mean = 3, sd = 1))
    y <- base + noise
  }
  
  if (model == 8) {
    noise <- ifelse(x < 5,
                    rnorm(n, mean = -2, sd = 1 + 0.5 * x),
                    rnorm(n, mean = 2, sd = 2 - 0.3 * (x - 5)))
    y <- base + noise
  }
  
  return(data.frame(x = x, y = y))
}


# 벡터의 L2 norm 계산 함수
compute_norm <- function(vector) {
  return(sqrt(sum(vector^2)))
}

# 근사로 k factor 계산하는 함수 
find_k_factor_fast <- function(nu, norm_lx_h, P = 0.90, gamma = 0.95) {
  sqrt(nu * qchisq(p = P, df = 1, ncp = norm_lx_h^2) / qchisq(p = 1 - gamma, df = nu))
}

# k factor 가져오기 (캐시 사용)
get_k_factor <- function(nu, norm_lx_h) {
  key <- as.character(round(norm_lx_h, 2))  # 소수점 2자리까지
  if (exists(key, envir = k_lookup_global)) {
    return(get(key, envir = k_lookup_global))
  } else {
    k_value <- find_k_factor_fast(nu, norm_lx_h)
    assign(key, k_value, envir = k_lookup_global)
    return(k_value)
  }
}

transform_and_evaluate_sequential <- function(models, sample_sizes, M, seed) {
  set.seed(seed)
  monte_carlo_results <- list()
  pointwise_coverage_result <- list()
  
  for (model in models) {
    for (m in sample_sizes) {
      cat(sprintf("\n[START] Model: %d, Sample size: %d\n", model, m))
      start_time_model <- Sys.time()
      
      lambda_list <- numeric(M)
      results_list <- list()
      
      for (i in 1:M) {
        data_i <- generate_data(model, m)
        x <- data_i$x
        y <- data_i$y
        
        fit <- tryCatch({
          smooth.spline(x = x, y = y, cv = TRUE)
        }, error = function(e) {
          warning(sprintf("⚠️ smoothing 실패: %s", e$message))
          return(NULL)
        })
        
        if (!is.null(fit) && !is.null(fit$lambda) && is.finite(fit$lambda)) {
          lambda_list[i] <- fit$lambda
        } else {
          lambda_list[i] <- NA
        }
      }
      
      mean_lambda <- mean(lambda_list, na.rm = TRUE)
      cat(sprintf("🌟 평균 lambda (Model %d, m=%d): %.5f\n", model, m, mean_lambda))
      
      for (i in 1:M) {
        set.seed(i + seed * 1000)
        data_i <- generate_data(model, m)
        train_x <- data_i$x
        train_y <- data_i$y
        
        set.seed(i + seed * 10000)
        test_data <- generate_data(model, m)
        test_x <- test_data$x
        test_y <- test_data$y
        
        fit <- smooth.spline(x = train_x, y = train_y, cv = FALSE, lambda = mean_lambda)
        pred_train <- predict(fit, train_x)$y
        
        if (model %in% c(2, 5, 8)) {
          residuals <- train_y - pred_train
          log_variance_fit <- smooth.spline(train_x, log(residuals^2 + 1e-6), cv = FALSE)
          variance_hat_train <- exp(predict(log_variance_fit, train_x)$y)
          
          transform_y_train <- train_y / sqrt(variance_hat_train)
          fit_trans <- smooth.spline(train_x, transform_y_train, cv = FALSE)
          
          B <- bs(train_x, df = fit_trans$df)
          D <- diff(diag(ncol(B)), differences = 2)
          S_inv <- ginv(t(B) %*% B + fit_trans$lambda * t(D) %*% D)
          S <- B %*% S_inv %*% t(B)
          I_n <- diag(length(train_x))
          R <- I_n - S
          
          residuals2 <- transform_y_train - predict(fit_trans, train_x)$y
          est_var <- (t(residuals2) %*% residuals2) / sum(diag(t(R) %*% R))
          est_var <- ifelse(is.na(est_var) | is.nan(est_var) | is.infinite(est_var), 1e-6, est_var)
          
          num <- sum(diag(t(R) %*% R))^2
          den <- sum(diag((t(R) %*% R)^2))
          nu <- num / den
          
          norm_lx_h_values <- sapply(1:ncol(S), function(j) compute_norm(S[, j]))
          k_factors <- sapply(norm_lx_h_values, function(nlh) get_k_factor(nu, nlh))
          
          TI_upper <- pred_train + sqrt(est_var) * k_factors
          TI_lower <- pred_train - sqrt(est_var) * k_factors
          TI_upper <- TI_upper * sqrt(variance_hat_train)
          TI_lower <- TI_lower * sqrt(variance_hat_train)
        } else {
          B <- bs(train_x, df = fit$df)
          D <- diff(diag(ncol(B)), differences = 2)
          S_inv <- ginv(t(B) %*% B + fit$lambda * t(D) %*% D)
          S <- B %*% S_inv %*% t(B)
          I_n <- diag(length(train_x))
          R <- I_n - S
          
          residuals2 <- train_y - predict(fit, train_x)$y
          est_var <- (t(residuals2) %*% residuals2) / sum(diag(t(R) %*% R))
          est_var <- ifelse(is.na(est_var) | is.nan(est_var) | is.infinite(est_var), 1e-6, est_var)
          
          num <- sum(diag(t(R) %*% R))^2
          den <- sum(diag((t(R) %*% R)^2))
          nu <- num / den
          
          norm_lx_h_values <- sapply(1:ncol(S), function(j) compute_norm(S[, j]))
          k_factors <- sapply(norm_lx_h_values, function(nlh) get_k_factor(nu, nlh))
          
          TI_upper <- pred_train + sqrt(est_var) * k_factors
          TI_lower <- pred_train - sqrt(est_var) * k_factors
        }
        
        covered <- (test_y >= TI_lower) & (test_y <= TI_upper)
        coverage_rate <- mean(covered)
        is_covered <- coverage_rate >= 0.90
        
        results_list[[i]] <- list(
          is_covered = is_covered,
          pointwise_covered = as.numeric(covered),
          mean_width = mean(TI_upper - TI_lower),
          NMPIW = mean((TI_upper - TI_lower) / (max(test_y) - min(test_y)))
        )
      }
      
      # ⏬ 루프 끝난 후 여기서 저장!
      empirical_coverage <- mean(sapply(results_list, function(res) res$is_covered))
      mean_width <- mean(sapply(results_list, function(res) res$mean_width))
      nmpiw <- mean(sapply(results_list, function(res) res$NMPIW))
      
      monte_carlo_results[[paste0("Model_", model, "_m_", m)]] <- 
        data.frame(
          Model = model,
          SampleSize = m,
          EmpiricalCoverage = empirical_coverage,
          MeanWidth = mean_width,
          NMPIW = nmpiw
        )
      
      pointwise_coverage_mean <- Reduce("+", lapply(results_list, function(res) res$pointwise_covered)) / M
      fixed_x <- seq(0, 10, length.out = m)
      
      pointwise_coverage_result[[paste0("Model_", model, "_m_", m)]] <- data.frame(
        x = fixed_x,
        coverage = pointwise_coverage_mean
      )
      
      end_time_model <- Sys.time()
      duration <- round(difftime(end_time_model, start_time_model, units = "mins"), 2)
      cat(sprintf("[END] Model: %d, Sample size: %d, — %.2f minutes\n", model, m, duration))
    }
  }
  
  cat("💾 k_lookup_global 캐시 저장 중...\n")
  saveRDS(k_lookup_global, k_lookup_path)
  
  return(list(
    monte_carlo_results = monte_carlo_results,
    pointwise_coverage_result = pointwise_coverage_result
  ))
}


seed=123
M=500
models <- 1:8
sample_sizes <- c(50, 100, 200, 500, 1000)

# 결과 저장 
k_lookup_global <- new.env()
k_lookup_path <- "k_lookup_global.rds"

monte_carlo_results <- list()
pointwise_coveraage_result <- list()
results <- transform_and_evaluate_sequential(models, sample_sizes, M, seed=seed)

monte_carlo_results <- results$monte_carlo_results
pointwise_coverage_result <- results$pointwise_coverage_result

summary_df <- do.call(rbind, lapply(names(monte_carlo_results), function(name) {
  df <- monte_carlo_results[[name]]
  
  data.frame(
    Model = as.integer(sub("Model_(\\d+)_.*", "\\1", name)),
    SampleSize = as.integer(sub(".*_m_(\\d+)", "\\1", name)),
    EmpiricalCoverage = round(df$EmpiricalCoverage, 4),
    MeanWidth = round(df$MeanWidth, 4),
    NMPIW = round(df$NMPIW, 4)
  )
}))

summary_df <- summary_df[order(summary_df$Model, summary_df$SampleSize), ]

write.csv(summary_df, "0521_final_paper_summary.csv", row.names = FALSE)





library(ggplot2)
library(dplyr)

# 저장 디렉토리 생성
dir.create("final_paper_pointwise_plots", showWarnings = FALSE)

# 모든 모델에 대해 pointwise plot 생성
for (name in names(pointwise_coverage_result)) {
  df <- pointwise_coverage_result[[name]]
  
  model <- as.integer(sub("Model_(\\d+)_m_.*", "\\1", name))
  m <- as.integer(sub(".*_m_(\\d+)", "\\1", name))
  
  p <- ggplot(df, aes(x = x, y = coverage)) +
    geom_line(color = "blue", alpha = 0.7) +
    geom_hline(yintercept = 0.90, linetype = "dashed", color = "red") +
    ylim(0, 1) +
    labs(
      title = sprintf("Model %d, Sample Size %d - Pointwise Coverage", model, m),
      x = "x", y = "Coverage"
    ) +
    theme_minimal(base_size = 14)
  
  ggsave(sprintf("paper_pointwise_plots/Model_%d_m_%d_pointwise.png", model, m),
         plot = p, width = 8, height = 5)
}
