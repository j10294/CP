library(splines)
library(foreach)
library(progressr)
library(doFuture)
library(future)
library(splines)
library(MASS)
library(parallel)
library(doRNG)

# 병렬 처리 설정
plan(multisession, workers = detectCores() - 1)
registerDoFuture()
handlers(global = TRUE)

# 🔥 1. k_lookup_global 캐시 설정
k_lookup_path <- "k_lookup_global.rds"

if (file.exists(k_lookup_path)) {
  cat("📂 k_lookup_global 캐시 불러오는 중...\n")
  k_lookup_global <- readRDS(k_lookup_path)
} else {
  cat("🆕 새로운 k_lookup_global 캐시 생성\n")
  k_lookup_global <- new.env()
}

# 🔥 2. 필요한 함수 정의

# 데이터 생성 함수 정의 (논문과 동일하게 수정)
generate_data <- function(model, n){
  if (model ==1 | model == 3){
    x <- seq(0,10, length.out=n)
  } else if (model==2 | model ==4) {
    x <- seq(-10, 10, length.out=n)
  }
  if (model == 1) {
    y <- 3 * cos(x) - 5 * (x / 15)^2 + rnorm(n, mean = 0, sd = sqrt(2))
  } else if (model == 2) {
    y <- (x / 5)^3 + rnorm(n, mean = 0, sd = 1)
  } else if (model == 3) {
    y <- 3 * cos(x) - 5 * (x / 15)^2 + rt(n, df = 3)
  } else if (model == 4) {
    y <- (x / 5)^3 + rt(n, df = 3)
  }
  
  return(data.frame(x=x, y=y))
}


# 벡터의 L2 norm 계산 함수
compute_norm <- function(vector) {
  return(sqrt(sum(vector^2)))
}

# Prop 3.1 내의 확률부분을 계산하는 함수 
compute_probability <- function(nu, t, P, k, norm_lx_h) {
  tryCatch({
    q_val <- qchisq(P, df = 1, ncp = t^2)
    if (!is.finite(q_val)) return(0)
    
    lhs <- q_val / ((t * sqrt(5 / nu)) + 1)
    if (!is.finite(lhs)) return(0)
    
    if (any(lhs <= k^2)) {
      threshold <- (nu * q_val) / (k^2)
      probability <- pchisq(threshold, df = nu, lower.tail = FALSE)
      if (!is.finite(probability) || is.na(probability) || is.nan(probability)) return(1e-6)
      return(probability)
    } else {
      return(0)
    }
  }, error = function(e) {
    warning("compute_probability 계산 오류 발생")
    return(0)
  })
}

# Prop 3.1 의 적분식을 계산하는 함수
integrand <- function(t, k, nu, P, norm_lx_h) {
  # 예외 처리
  if (!is.numeric(t) || !is.finite(t) || t <= 0) return(0)
  if (!is.numeric(norm_lx_h) || norm_lx_h <= 1e-8) return(0)
  
  exp_term <- exp(-t^2 / (2 * norm_lx_h^2))
  if (!is.finite(exp_term) || exp_term < 1e-10) return(0)
  
  prob_term <- compute_probability(nu, t, P, k, norm_lx_h)
  if (!is.finite(prob_term) || prob_term < 1e-10) return(0)
  
  return(exp_term * prob_term)
}


find_upper_bound_t <- function(k, nu, P) {
  t_seq <- seq(0.01, 100, by = 0.05)  # 탐색 구간
  valid_t <- sapply(t_seq, function(t) {
    q_val <- qchisq(P, df = 1, ncp = t^2)
    lhs <- q_val / ((t * sqrt(5 / nu)) + 1)
    return(lhs <= k^2)
  })
  
  if (any(valid_t)) {
    return(max(t_seq[valid_t]))
  } else {
    warning("조건을 만족하는 t가 없음 — fallback to t = 10")
    return(10)
  }
}

# k-factor 계산 함수
find_k_factor <- function(nu, norm_lx_h, P = 0.90, gamma = 0.95, index = NA) {
  start_time <- Sys.time()
  cat("▶️ k-factor 계산 시작: index =", index, "\n")
  
  obj_func <- function(k) {
    upper_bound <- find_upper_bound_t(k, nu, P)
    
    if (is.na(upper_bound) || is.infinite(upper_bound) || upper_bound < 1e-6) {
      warning("upper_bound가 이상함 → fallback")
      return(Inf) #upperbound 상한값을 inf로 설정
    }
    
    result <- tryCatch({
      integral_result <- integrate(integrand, lower = 0, upper = upper_bound,
                                   k = k, nu = nu, P = P, norm_lx_h = norm_lx_h)$value
      if (is.na(integral_result) || is.nan(integral_result)) return(Inf)
      pi_term <- sqrt(2 / (pi * norm_lx_h^2))
      error <- abs((pi_term * integral_result) - gamma)
      
      cat("  ⏳ k =", round(k, 5), 
          ", upper bound of t ", upper_bound, 
          ", integral =", round(pi_term * integral_result, 5), 
          ", error =", round(error, 5), "\n")
      
    }, error = function(e) {
      warning("적분 실패")
      return(Inf)
    })
    
    return(result)
  }
  
  
  tryCatch({
    return(optimize(obj_func, interval = c(0.01, 5), tol = 1e-2)$minimum)
    elapsed <- Sys.time() - start_time
    cat("✅ index =", index, ", 최적 k =", round(out, 5), ", 시간:", round(elapsed, 2), "초\n")
    
  }, error = function(e) {
    warning("k 최적화 실패")
    #return(sqrt(nu*qchisq(p=0.90, df=1, ncp=norm_lx_h^2)/qchisq(p=1-gamma, df=nu, ncp=0))) # 최적화 실패 시 근사값 사용
  })
}

# 근사로 k factor 계산하는 함수 
find_k_factor_fast <- function(nu, norm_lx_h, P = 0.90, gamma = 0.95) {
  sqrt(nu * qchisq(p = P, df = 1, ncp = norm_lx_h^2) / qchisq(p = 1 - gamma, df = nu))
}

# k factor 가져오기 (캐시 사용)
get_k_factor <- function(nu, norm_lx_h) {
  key <- as.character(round(norm_lx_h, 2))  # 소수점 2자리까지
  if (exists(key, envir = k_lookup_global)) {
    return(get(key, envir = k_lookup_global))
  } else {
    k_value <- find_k_factor_fast(nu, norm_lx_h)
    assign(key, k_value, envir = k_lookup_global)
    return(k_value)
  }
}

# 데이터 변환 함수 
transform_function <- function(models, sample_sizes, M, seed) {
  set.seed(seed)
  transformed_data_list <- list()
  raw_data_list <- list()
  
  for (model in models) {
    for (m in sample_sizes) {
      transformed_data <- matrix(NA, nrow = m, ncol = M)
      raw_data <- vector("list", M)
      
      # 데이터의 분산을 추정해 추정된 분산으로 데이터를 나누어 transform
      for (i in 1:M) {
        data <- generate_data(model, m)
        x <- data$x
        y <- data$y
        fit <- smooth.spline(x = x, y = y, cv = FALSE) #논문 방법과 동일하게 GCV 사용
        pred_y <- predict(fit, x)$y
        residuals <- y - pred_y
        
        log_variance_fit <- smooth.spline(x = x, y = log(residuals^2 + 1e-6), cv = FALSE)
        log_variance_hat <- predict(log_variance_fit, x)$y
        variance_hat <- exp(log_variance_hat)
        
        variance_hat[is.nan(variance_hat) | is.infinite(variance_hat)] <- 1e-6
        transform_y <- y / sqrt(variance_hat)
        
        transformed_data[, i] <- transform_y
        raw_data[[i]] <- data.frame(x = x, y = y, variance_hat = variance_hat)
      }
      
      transformed_data_list[[paste0("Model_", model, "_m_", m)]] <- transformed_data
      raw_data_list[[paste0("Model_", model, "_m_", m)]] <- raw_data
    }
  }
  
  return(list(transformed = transformed_data_list, raw = raw_data_list))
}

# 시각화 함수
plot_ti <- function(x, y, TI_lower, TI_upper, pred_mean, varname, covered, picp, nmpiw, cwc) {
  df <- data.frame(
    x = x, y = y,
    lower = TI_lower, upper = TI_upper,
    pred = pred_mean, covered = covered
  )
  p <- ggplot(df, aes(x = x, y = y)) +
    geom_point(aes(color = covered), size = 1.5) +
    geom_ribbon(aes(ymin = lower, ymax = upper), fill = "green", alpha = 0.3) +
    geom_line(aes(y = pred), color = "darkgreen", size = 0.7) +
    labs(
      title = paste0("TI for ", varname),
      subtitle = sprintf("PICP=%.3f, NMPIW=%.3f, CWC=%.3f", picp, nmpiw, cwc),
      x = "x", y = varname
    ) +
    theme_minimal(base_size = 11) +
    scale_color_manual(values = c("TRUE" = "blue", "FALSE" = "red"))
  return(p)
}


# 🔥 3. 메인 함수 (transform and evaluate)
transform_and_evaluate <- function(models, sample_sizes, M, seed) {
  set.seed(seed)
  monte_carlo_results <- list()
  
  for (model in models) {
    for (m in sample_sizes) {
      cat(sprintf("\n[START] Model: %d, Sample size: %d\n", model, m))
      start_time_model <- Sys.time()
      
      lambda_list <- numeric(M)   # 🔥 추가: GCV로 구한 lambda 저장할 벡터
      
      # 1차 패스: GCV로 각 실험별 lambda 저장
      for (i in 1:M) {
        data_i <- generate_data(model, m)
        x <- data_i$x
        y <- data_i$y
        
        fit <- smooth.spline(x = x, y = y, cv = TRUE)   # 🔥 GCV 적용
        lambda_list[i] <- fit$lambda   # 🔥 lambda 저장
      }
      
      mean_lambda <- mean(lambda_list)  # 🔥 평균 lambda 계산
      cat(sprintf("🌟 평균 lambda (Model %d, m=%d): %.5f\n", model, m, mean_lambda))
      
      with_progress({
        p <- progressor(steps = M)
        start_time_loop <- Sys.time()
        
        results_list <- foreach(i = 1:M, .packages = c('splines', 'MASS', 'foreach', 'doParallel','progressr'),
                                .export = c('generate_data', 'compute_norm','compute_probability','find_upper_bound_t','integrand','find_k_factor','get_k_factor', 'find_k_factor_fast','transform_function','plot_ti')) %dorng% {
          set.seed(i+seed*1000) #중복 방지용 
          p(sprintf('Seed %d', i))                        
          
          # 남은 시간 예측
          elapsed <- as.numeric(difftime(Sys.time(), start_time_loop, units = "secs"))
          est_remain_min <- round((elapsed / i * M - elapsed) / 60, 2)
          p(sprintf("Progress: %d/%d | 예상 남은 시간: %.2f 분", i, M, est_remain_min))
          
          data_i <- generate_data(model, m)
          x <- data_i$x
          y <- data_i$y
          
          train_x <- x; train_y <- y
          test_x <- x; test_y <- y
          
          # 🔥 평균 lambda를 사용해서 smoothing
          fit <- smooth.spline(x = train_x, y = train_y, cv = FALSE, lambda = mean_lambda)
          pred_train <- predict(fit, train_x)$y
          residuals <- train_y - pred_train
          
          # variance estimate
          log_variance_fit <- smooth.spline(train_x, log(residuals^2 + 1e-6), cv = FALSE)
          variance_hat_train <- exp(predict(log_variance_fit, train_x)$y)
          
          # transform
          transform_y_train <- train_y / sqrt(variance_hat_train)
          fit_trans <- smooth.spline(train_x, transform_y_train, cv = FALSE)
          
          # Smoother matrix using QR decomposition
          B <- bs(train_x, df = fit_trans$df)
          D <- diff(diag(ncol(B)), differences = 2)
          S_inv <- ginv(t(B) %*% B + fit_trans$lambda * t(D) %*% D)
          S <- B %*% S_inv %*% t(B)
          I_n <- diag(length(train_x))
          R <- I_n - S
          
          # residual variance estimate
          residuals2 <- transform_y_train - predict(fit_trans, train_x)$y
          est_var <- (t(residuals2) %*% residuals2) / sum(diag(t(R) %*% R))
          est_var <- ifelse(is.na(est_var) | is.nan(est_var) | is.infinite(est_var), 1e-6, est_var)
          
          # effective degrees of freedom
          num <- sum(diag(t(R) %*% R))^2
          den <- sum(diag((t(R) %*% R)^2))
          nu <- num / den
          
          # k-factor 계산
          norm_lx_h_values <- sapply(1:ncol(S), function(j) compute_norm(S[, j]))
          k_factors <- sapply(norm_lx_h_values, function(nlh) get_k_factor(nu, nlh))
          
          # Tolerance interval
          TI_upper <- pred_train + sqrt(est_var) * k_factors
          TI_lower <- pred_train - sqrt(est_var) * k_factors
          TI_upper <- TI_upper * sqrt(variance_hat_train)
          TI_lower <- TI_lower * sqrt(variance_hat_train)
          
          coverage <- (test_y >= TI_lower) & (test_y <= TI_upper)
          width <- TI_upper - TI_lower
          
          
          if (i == 1) {
            library(ggplot2)
            
            RZ <- residuals2
            observed <- RZ^2
            theoretical <- qchisq(ppoints(length(observed)), df = nu)
            
            ## 파일명 자동화
            base_filename <- sprintf("Model%d_m%d_seed%d", model, m, seed)
            
            ## (1) QQ plot 저장 (paper, figure 1)
            png(filename = paste0("qqplot_", base_filename, ".png"), width = 800, height = 600)
            
            df_qq <- data.frame(
              theoretical = theoretical,
              observed = sort(observed)
            )
            
            p1 <- ggplot(df_qq, aes(x = theoretical, y = observed)) +
              geom_point(size = 1) +
              geom_abline(slope = 1, intercept = 0, color = "red", linetype = "solid", size = 1) +
              labs(
                title = paste0("QQ Plot: Model ", model, ", m=", m),
                x = "Theoretical Quantiles (Chi-square)",
                y = "Observed Squared Residuals"
              ) +
              theme_bw(base_size = 14)
            
            print(p1)
            dev.off()
            
            ## (2) 히스토그램 + Chi-square density 저장 (paper, figure 2) 
            png(filename = paste0("hist_chisq_", base_filename, ".png"), width = 800, height = 600)
            
            x_vals <- seq(0, max(observed)*1.2, length.out = 1000)
            theoretical_density <- dchisq(x_vals, df = nu)
            
            df_hist <- data.frame(
              observed = observed
            )
            df_theoretical <- data.frame(
              x = x_vals,
              density = theoretical_density
            )
            
            p2 <- ggplot(df_hist, aes(x = observed)) +
              geom_histogram(aes(y = ..density..), bins = 30, fill = "grey80", color = "white") +
              geom_line(data = df_theoretical, aes(x = x, y = density), color = "red", size = 1.2) +
              labs(
                title = paste0("Histogram + Chi-square: Model ", model, ", m=", m),
                x = "Squared Residuals",
                y = "Density"
              ) +
              theme_bw(base_size = 14)
            
            print(p2)
            dev.off()
          }
          
          list(coverage = coverage, width = width, test_x = test_x, test_y = test_y)
        }
      })
      
      # 결과 정리
      coverage_matrix <- do.call(cbind, lapply(results_list, function(res) res$coverage))
      width_matrix <- do.call(cbind, lapply(results_list, function(res) res$width))
      test_x <- results_list[[1]]$test_x
      test_y <- results_list[[1]]$test_y
      
      coverage_probabilities <- rowMeans(coverage_matrix)
      range_y <- max(test_y) - min(test_y)
      normalized_width <- width_matrix / range_y
      nmpiw <- rowMeans(normalized_width)
      mean_width <- rowMeans(width_matrix)
      
      monte_carlo_results[[paste0("Model_", model, "_m_", m)]] <- 
        data.frame(x = sort(test_x), coverage_probabilities = coverage_probabilities, mean_width = mean_width, NMPIW = nmpiw)
      
      end_time_model <- Sys.time()
      duration <- round(difftime(end_time_model, start_time_model, units = "mins"), 2)
      cat(sprintf("[END] Model: %d, Sample size: %d — %.2f minutes\n", model, m, duration))
    }
  }
  
  # 🔥 캐시 저장
  cat("💾 k_lookup_global 캐시 저장 중...\n")
  saveRDS(k_lookup_global, k_lookup_path)
  
  return(monte_carlo_results)
}

seed = 123
M=500
models <- c(1, 2, 3, 4)
sample_sizes <- c(50, 100, 200)

# 결과 저장 
monte_carlo_results <- list()
monte_carlo_results <- transform_and_evaluate(models, sample_sizes, M, seed=seed)


# 결과를 하나의 데이터프레임으로 정리
results <- monte_carlo_results
summary_df <- do.call(rbind, lapply(names(results), function(name) {
  df <- monte_carlo_results[[name]]
  df_summary <- data.frame(
    Model = sub("Model_(\\d+)_.*", "\\1", name),
    SampleSize = sub("._m_(\\d+)", "\\1", name),
    PICP = round(mean(df$coverage_probabilities, na.rm=TRUE),4),
    NMPIW = round(mean(df$NMPIW, na.rm=TRUE),4)
  )
  return(df_summary)
}))

summary_df
summary_df$SampleSize <- as.integer(gsub("Model_", "", summary_df$SampleSize))
summary_df$Model <- as.integer(summary_df$Model)
summary_df <- summary_df[order(summary_df$Model, summary_df$SampleSize), ]

library(knitr)
kable(summary_df, caption='Paper 모델 그대로 (k 근사) ')





# 전체 모델별, 샘플 사이즈별 plot
library(ggplot2)
library(dplyr)

pointwise_coverage_summary <- do.call(rbind, lapply(names(monte_carlo_results), function(name) {
  df <- monte_carlo_results[[name]]
  
  if (!("coverage_probabilities" %in% names(df))) return(NULL)
  
  df <- df[!is.na(df$coverage_probabilities), ]  # NA 제거
  if (nrow(df) == 0) return(NULL)  # 전부 NA면 skip
  
  model <- as.integer(sub("Model_(\\d+)_m_.*", "\\1", name))
  m <- as.integer(sub(".*_m_(\\d+)", "\\1", name))
  
  data.frame(
    Model = model,
    SampleSize = m,
    x = df$x,
    Coverage = df$coverage_probabilities,
    mean_width = df$mean_width,
    NMPIW = df$NMPIW
  )
}))

picp_nmpiw_summary <- pointwise_coverage_summary %>%
  group_by(Model, SampleSize) %>%
  summarise(
    PICP = round(mean(Coverage, na.rm = TRUE), 3),
    NMPIW = round(mean(NMPIW, na.rm = TRUE), 3)
  )

plot_data <- pointwise_coverage_summary %>%
  left_join(picp_nmpiw_summary, by = c("Model", "SampleSize"))

ggplot(plot_data, aes(x = x, y = Coverage)) +
  geom_point(color = "blue", size=0.7) +
  geom_hline(yintercept = 0.95, linetype = "dashed", color = "black", size=1.2) +
  facet_grid(Model ~ SampleSize,scales='free_x', labeller = label_both) +
  theme_bw(base_size = 14) +
  labs(title = "Pointwise Coverage Probability with PICP & NMPIW (Young, 2013)",
       x = "x",
       y = "Coverage Probability") +
  geom_text(data = picp_nmpiw_summary,
            aes(x = Inf, y = -0.05, label = paste0("PICP=", PICP, "\nNMPIW=", NMPIW)),
            inherit.aes = FALSE,
            hjust = 1.1, vjust = 0, size = 3.5, color = "black")




#### find_k_factor VS find_k_factor_fast ####
# 비교 함수 정의
compare_k_factors <- function(nu = 100, P = 0.9, gamma = 0.95, n_points = 50, seed = 42) {
  set.seed(seed)
  
  # norm_lx_h 값을 50개 생성 (0.5 ~ 5 사이 균등 분포)
  norm_lx_h_values <- sort(runif(n_points, min = 0.5, max = 5))
  
  # 두 함수 결과 저장
  k_exact <- numeric(n_points)
  k_fast  <- numeric(n_points)
  
  for (i in seq_along(norm_lx_h_values)) {
    nlh <- norm_lx_h_values[i]
    
    #k_fast[i] <- find_k_factor_fast(nu, nlh, P = P, gamma = gamma) 근사사용~
    k_fast[i] <- find_k_factor(nu, nlh, P=P, gamma=gamma)

    # tryCatch로 실패 방어
    k_exact[i] <- tryCatch(
      find_k_factor(nu, nlh, P = P, gamma = gamma),
      error = function(e) NA
    )
  }
  
  # 차이 계산
  diff_k <- k_exact - k_fast
  rel_diff <- diff_k / k_exact
  
  # 데이터프레임 반환
  result_df <- data.frame(
    norm_lx_h = norm_lx_h_values,
    k_exact = k_exact,
    k_fast = k_fast,
    abs_diff = diff_k,
    rel_diff = rel_diff
  )
  
  return(result_df)
}
