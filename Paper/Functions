library(splines)
library(foreach)
library(doParallel)


#  CPU 코어 개수 확인 및 병렬 클러스터 설정
num_cores <- detectCores() - 1 
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# 1. 데이터 생성 함수 정의
generate_data <- function(model, n) {
  x <- seq(0, 10, length.out = n)
  pop <- 3 * cos(x) - 5 * (x / 15)^2
  if (model == 1) {
    y <- 3 * cos(x) - 5 * (x / 15)^2 + rnorm(n, mean = 0, sd = sqrt(2))
  }
  if (model == 2) {
    y <- 3 * cos(x) - 5 * (x / 15)^2 + rnorm(n, mean = 0, sd = sqrt(2 * x))
  }
  if (model == 3) {
    y <- 3 * cos(x) - 5 * (x / 15)^2 + 4 - rgamma(n, shape = 2, scale = 2)
  }
  if (model == 4) {
    y <- 3 * cos(x) - 5 * (x / 15)^2 + rt(n, df = 3)
  }
  return(data.frame(x = x, y = y, pop = pop))
}

# 2. 벡터의 L2 norm 계산 함수
compute_norm <- function(vector) {
  return(sqrt(sum(vector^2)))
}

# 3. Prop 3.1 내의 확률부분을 계산하는 함수 
compute_probability <- function(nu, t, P, k, norm_lx_h) {
  tryCatch({
    q_val <- qchisq(P, df = 1, ncp = t^2) #qchisq value 설정
    lhs <- q_val / ((t * sqrt(5 / nu)) + 1) #이 값이 k^2보다 크면, probability가 0으로 감.
    
    if (lhs <= k^2) {
      threshold <- (nu * q_val) / (k^2)
      probability <- pchisq(threshold, df = nu, lower.tail = FALSE) #probability term
      
      if (is.na(probability) || is.nan(probability)) return(1e-6)
      return(probability)
    } else {
      return(0) #이 값이 k^2보다 크면, probability가 0으로 감.
    }
  }, error = function(e) {
    warning("compute_probability 계산 오류 발생")
    return(0)
  })
}

# 4. Prop 3.1 의 적분식을 계산하는 함수
integrand <- function(t, k, nu, P, norm_lx_h) {
  exp_term <- exp(-t^2 / (2 * norm_lx_h^2))
  if (exp_term < 1e-5) return(0) #exp term 이 너무 작으면 그냥 0 반환
  
  prob_term <- compute_probability(nu, t, P, k, norm_lx_h)
  return(exp_term * prob_term)
}

find_upper_bound_t <- function(k, nu, P) {
  t_seq <- seq(0.01, 100, by = 0.05)  # 탐색 구간
  valid_t <- sapply(t_seq, function(t) {
    q_val <- qchisq(P, df = 1, ncp = t^2)
    lhs <- q_val / ((t * sqrt(5 / nu)) + 1)
    return(lhs <= k^2)
  })
  
  if (any(valid_t)) {
    return(max(t_seq[valid_t]))
  } else {
    warning("조건을 만족하는 t가 없음 — fallback to t = 10")
    return(10)
  }
}

# k-factor 계산 함수
find_k_factor <- function(nu, norm_lx_h, P = 0.90, gamma = 0.95, index = NA) {
  start_time <- Sys.time()
  cat("▶️ k-factor 계산 시작: index =", index, "\n")
  
  obj_func <- function(k) {
    upper_bound <- find_upper_bound_t(k, nu, P)
    
    if (is.na(upper_bound) || is.infinite(upper_bound) || upper_bound < 1e-6) {
      warning("upper_bound가 이상함 → fallback")
      return(Inf) #upperbound 상한값을 inf로 설정
    }
    
    result <- tryCatch({
      integral_result <- integrate(integrand, lower = 0, upper = upper_bound,
                                   k = k, nu = nu, P = P, norm_lx_h = norm_lx_h)$value
      if (is.na(integral_result) || is.nan(integral_result)) return(Inf)
      pi_term <- sqrt(2 / (pi * norm_lx_h^2))
      error <- abs((pi_term * integral_result) - gamma)
      
      cat("  ⏳ k =", round(k, 5), 
          ", upper bound of t ", upper_bound, 
          ", integral =", round(pi_term * integral_result, 5), 
          ", error =", round(error, 5), "\n")
      
    }, error = function(e) {
      warning("적분 실패")
      return(Inf)
    })
    
    return(result)
  }
  
  
  tryCatch({
    return(optimize(obj_func, interval = c(0.01, 5), tol = 1e-2)$minimum)
    elapsed <- Sys.time() - start_time
    cat("✅ index =", index, ", 최적 k =", round(out, 5), ", 시간:", round(elapsed, 2), "초\n")
    
  }, error = function(e) {
    warning("k 최적화 실패")
    return(sqrt(nu*qchisq(p=0.90, df=1, ncp=norm_lx_h^2)/qchisq(p=1-gamma, df=nu, ncp=0))) # 최적화 실패 시 근사값 사용
  })
}

# 6. 데이터 변환 함수 
transform_function <- function(models, sample_sizes, M, seed) {
  set.seed(seed)
  transformed_data_list <- list()
  raw_data_list <- list()
  
  for (model in models) {
    for (m in sample_sizes) {
      transformed_data <- matrix(NA, nrow = m, ncol = M)
      raw_data <- vector("list", M)
      
      # 데이터의 분산을 추정해 추정된 분산으로 데이터를 나누어 transform
      for (i in 1:M) {
        data <- generate_data(model, m)
        x <- data$x
        y <- data$y
        fit <- smooth.spline(x = x, y = y, cv = FALSE) #논문 방법과 동일하게 GCV 사용
        pred_y <- predict(fit, x)$y
        residuals <- y - pred_y
        
        log_variance_fit <- smooth.spline(x = x, y = log(residuals^2 + 1e-6), cv = FALSE)
        log_variance_hat <- predict(log_variance_fit, x)$y
        variance_hat <- exp(log_variance_hat)
        
        variance_hat[is.nan(variance_hat) | is.infinite(variance_hat)] <- 1e-6
        transform_y <- y / sqrt(variance_hat)
        
        transformed_data[, i] <- transform_y
        raw_data[[i]] <- data.frame(x = x, y = y, variance_hat = variance_hat)
      }
      
      transformed_data_list[[paste0("Model_", model, "_m_", m)]] <- transformed_data
      raw_data_list[[paste0("Model_", model, "_m_", m)]] <- raw_data
    }
  }
  
  return(list(transformed = transformed_data_list, raw = raw_data_list))
}

# 7. 시각화 함수
plot_ti <- function(x, y, TI_lower, TI_upper, pred_mean, varname, covered, picp, nmpiw, cwc) {
  df <- data.frame(
    x = x, y = y,
    lower = TI_lower, upper = TI_upper,
    pred = pred_mean, covered = covered
  )
  p <- ggplot(df, aes(x = x, y = y)) +
    geom_point(aes(color = covered), size = 1.5) +
    geom_ribbon(aes(ymin = lower, ymax = upper), fill = "green", alpha = 0.3) +
    geom_line(aes(y = pred), color = "darkgreen", size = 0.7) +
    labs(
      title = paste0("TI for ", varname),
      subtitle = sprintf("PICP=%.3f, NMPIW=%.3f, CWC=%.3f", picp, nmpiw, cwc),
      x = "x", y = varname
    ) +
    theme_minimal(base_size = 11) +
    scale_color_manual(values = c("TRUE" = "blue", "FALSE" = "red"))
  return(p)
}
