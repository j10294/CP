library(splines)
library(foreach)
library(doParallel)


#  CPU 코어 개수 확인 및 병렬 클러스터 설정
num_cores <- detectCores() - 1 
cl <- makeCluster(num_cores)
registerDoParallel(cl)

library(MASS)


monte_carlo_results <- list()

transform_and_evaluate <- function(models, sample_sizes, M, seed) {
  set.seed(seed)
  monte_carlo_results <- list()
  
  for (model in models) {
    for (m in sample_sizes) {
      cat(sprintf("\n[START] Model: %d, Sample size: %d\n", model, m))
      start_time <- Sys.time()
      
      k_lookup_table <- new.env()
      
      # 병렬 처리 
      results_list <- foreach(i = 1:M, .packages=c('splines', 'MASS'),
                              .export = c("generate_data", "find_k_factor", "find_upper_bound_t", 
                                          "find_k_factor_fast", "compute_probability", "integrand", "compute_norm")
                              ) %dopar% {
        
        data_i <- generate_data(model, m)
        x <- data_i$x
        y <- data_i$y
        pop <- data_i$pop
        
        idx <- sample(1:m, size = m / 2)
        train_x <- x[idx]; train_y <- y[idx]
        test_x <- x[-idx]; test_y <- y[-idx]
        
        fit <- smooth.spline(x = train_x, y = train_y, cv = FALSE)
        pred_train <- predict(fit, train_x)$y
        residuals <- train_y - pred_train
        
        log_variance_fit <- smooth.spline(x = train_x, y = log(residuals^2 + 1e-6), cv = FALSE)
        variance_hat_train <- exp(predict(log_variance_fit, train_x)$y)
        
        transform_y_train <- train_y / sqrt(variance_hat_train)
        fit_trans <- smooth.spline(x = train_x, y = transform_y_train, cv = FALSE)
        
        B <- bs(train_x, df = fit_trans$df)
        D <- diff(diag(ncol(B)), differences = 2)
        S_inv <- ginv(t(B) %*% B + fit_trans$lambda * t(D) %*% D)
        S <- B %*% S_inv %*% t(B)
        I_n <- diag(length(train_x))
        R <- I_n - S
        
        residuals <- transform_y_train - predict(fit_trans, train_x)$y
        est_var <- (t(residuals) %*% residuals) / sum(diag(t(R) %*% R))
        est_var <- ifelse(is.na(est_var) | is.nan(est_var) | is.infinite(est_var), 1e-6, est_var)
        
        num <- sum(diag(t(R) %*% R))^2
        den <- sum(diag((t(R) %*% R)^2))
        nu <- num / den
        
        norm_lx_h_values <- sapply(1:ncol(S), function(j) { sqrt(sum(S[, j]^2)) })
        
        k_factors <- sapply(norm_lx_h_values, function(nlh) {
          rounded_nlh <- round(nlh, 2)
          key <- as.character(rounded_nlh)
          if (!exists(key, envir = k_lookup_table)) {
            k_value <- find_k_factor(nu = nu, norm_lx_h = rounded_nlh)
            assign(key, k_value, envir = k_lookup_table)
          }
          get(key, envir = k_lookup_table)
        })
        
        TI_upper <- pred_train + sqrt(est_var) * k_factors
        TI_lower <- pred_train - sqrt(est_var) * k_factors
        TI_upper <- TI_upper * sqrt(variance_hat_train)
        TI_lower <- TI_lower * sqrt(variance_hat_train)
        
        # 선형 보간 
        TI_lower_df <- aggregate(TI_lower ~ train_x, FUN = mean)
        TI_upper_df <- aggregate(TI_upper ~ train_x, FUN = mean)
        interp_lower <- approx(x = TI_lower_df$train_x, y = TI_lower_df$TI_lower, xout = test_x)$y
        interp_upper <- approx(x = TI_upper_df$train_x, y = TI_upper_df$TI_upper, xout = test_x)$y
        
        coverage <- (test_y >= interp_lower) & (test_y <= interp_upper)
        width <- TI_upper - TI_lower
        
        list(coverage = coverage, width = width, test_x = test_x, test_y = test_y)
      }
      
      # 결과 합치기
      coverage_matrix <- do.call(cbind, lapply(results_list, function(res) res$coverage))
      width_matrix <- do.call(cbind, lapply(results_list, function(res) res$width))
      test_x <- results_list[[1]]$test_x  # 테스트 x는 같으니까 하나만
      test_y <- results_list[[1]]$test_y
      
      end_time <- Sys.time()
      duration <- round(difftime(end_time, start_time, units = "mins"), 2)
      cat(sprintf("[END] Model: %d, Sample size: %d — %.2f minutes\n", model, m, duration))
      
      # PICP 계산
      coverage_probabilities <- rowMeans(coverage_matrix)
      
      # NMPIW 계산
      range_y <- max(test_y) - min(test_y)
      normalized_width <- width_matrix / range_y
      nmpiw <- rowMeans(normalized_width)
      
      # Mean Width
      mean_width <- rowMeans(width_matrix)
      
      monte_carlo_results[[paste0("Model_", model, "_m_", m)]] <- 
        data.frame(x = sort(test_x), coverage_probabilities = coverage_probabilities, mean_width = mean_width, NMPIW = nmpiw)
    }
  }
  
  return(monte_carlo_results)
}

seed = 123
M=500
models <- c(1, 2, 3, 4)
sample_sizes <- c(50, 100, 200)

# 결과 저장 
monte_carlo_results <- transform_and_evaluate(models, sample_sizes, M, seed=seed)

# 결과를 하나의 데이터프레임으로 정리
results <- monte_carlo_results
summary_df <- do.call(rbind, lapply(names(results), function(name) {
  df <- monte_carlo_results[[name]]
  df_summary <- data.frame(
    Model = sub("Model_(\\d+)_.*", "\\1", name),
    SampleSize = sub("._m_(\\d+)", "\\1", name),
    PICP = round(mean(df$coverage_probabilities, na.rm=TRUE),4),
    NMPIW = round(mean(df$NMPIW, na.rm=TRUE),4)
  )
  return(df_summary)
}))

summary_df
summary_df$SampleSize <- as.integer(gsub("Model_", "", summary_df$SampleSize))
summary_df$Model <- as.integer(summary_df$Model)
summary_df <- summary_df[order(summary_df$Model, summary_df$SampleSize), ]

library(knitr)
kable(summary_df, caption='(Paper) 모델별, 샘플 크기별 PICP, NMPIW (k 직접 계산) ')
